[[install-config-configuring-sdn]]
= Configuring the SDN
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[OpenShift SDN] enables
communication between pods across the {product-title} cluster, establishing a _pod
network_. Three xref:../architecture/networking/sdn.adoc#architecture-additional-concepts-sdn[SDN plug-ins]
are currently available (*ovs-subnet*, *ovs-multitenant*, and *ovs-networkpolicy*), which provide
different methods for configuring the pod network.

[[admin-guide-configuring-sdn-available-sdn-providers]]
== Available SDN Providers

The upstream Kubernetes project does not come with a default network solution.
Instead, Kubernetes has developed a Container Network Interface (CNI) to allow
network providers for integration with their own SDN solutions.

There are several OpenShift SDN plugins available out of the box from Red Hat,
as well as third-party plug-ins.

Red Hat has worked with a number of SDN providers to certify their SDN network
solution on {product-title} via the Kubernetes CNI interface, including a
support process for their SDN plug-in through their product’s entitlement
process. Should you open a support case with OpenShift, Red Hat can facilitate
an exchange process so that both companies are involved in meeting your needs.

The following SDN solutions are validated and supported on {product-title}
directly by the 3rd party vendor:

* Cisco Contiv (™)
* Juniper Contrail (™)
* Nokia Nuage (™)
* Tigera Calico (™)
* VMware NSX-T (™)

[discrete]
**Installing VMware NSX-T (™) on {product-title}**

VMware NSX-T (™) provides an SDN and security infrastructure to build
cloud-native application environments. In addition to vSphere hypervisors (ESX),
these environments include KVM and native public clouds.

The current integration requires a _new_ install of both NSX-T and
{product-title}. Currently, NSX-T version 2.0 is supported, and only supports
the use of ESX and KVM hypervisors at this time.

See the
link:https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_openshift.pdf[NSX-T
Container Plug-in for OpenShift - Installation and Administration Guide] for
more information.


[[configuring-sdn-config-pod-network-ansible]]
== Configuring the Pod Network with Ansible

For initial xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[advanced installations],
the *ovs-subnet* plug-in is installed and configured by default, though it can
be
xref:../install_config/install/advanced_install.adoc#configuring-ansible[overridden during installation]
using the
xref:../install_config/install/advanced_install.adoc#configuring-cluster-variables[`*os_sdn_network_plugin_name*` parameter],
which is configurable in the Ansible inventory file.

.Example SDN Configuration with Ansible
====

----
# Configure the multi-tenant SDN plugin (default is 'redhat/openshift-ovs-subnet')
# os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'

# Configure the NetworkPolicy SDN plugin
# os_sdn_network_plugin_name='redhat/openshift-ovs-networkpolicy'

# Disable the OpenShift SDN plugin
# openshift_use_openshift_sdn=False

# Configure SDN cluster network CIDR block. This network block should
# be a private block and should not conflict with existing network
# blocks in your infrastructure that pods may require access to.
# Can not be changed after deployment.
#osm_cluster_network_cidr=10.1.0.0/16

# default subdomain to use for exposed routes
#openshift_master_default_subdomain=apps.test.example.com

# Configure SDN cluster network and kubernetes service CIDR blocks. These
# network blocks should be private and should not conflict with network blocks
# in your infrastructure that pods may require access to. Can not be changed
# after deployment.
#osm_cluster_network_cidr=10.1.0.0/16
#openshift_portal_net=172.30.0.0/16

# Configure number of bits to allocate to each host’s subnet e.g. 8
# would mean a /24 network on the host.
#osm_host_subnet_length=8

# This variable specifies the service proxy implementation to use:
# either iptables for the pure-iptables version (the default),
# or userspace for the userspace proxy.
#openshift_node_proxy_mode=iptables
----
====

ifdef::openshift-enterprise[]
For initial xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[quick installations],
the *ovs-subnet* plug-in is installed and configured by default as well, and can
be
xref:../install_config/master_node_configuration.adoc#master-configuration-files[reconfigured post-installation]
using the `*networkConfig*` stanza of the *_master-config.yaml_* file.
endif::[]

[[configuring-the-pod-network-on-masters]]
== Configuring the Pod Network on Masters

The cluster administrators can control pod network settings on masters by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[master configuration file]
(located at *_/etc/origin/master/master-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  clusterNetworkCIDR: 10.128.0.0/14 <1>
  hostSubnetLength: 9 <2>
  networkPluginName: "redhat/openshift-ovs-subnet" <3>
  serviceNetworkCIDR: 172.30.0.0/16 <4>
----
<1> Cluster network for node IP allocation
<2> Number of bits for pod IP allocation within a node
<3> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
<4> Service IP allocation for the cluster
====

[IMPORTANT]
====
The `*serviceNetworkCIDR*` and `*hostSubnetLength*` values cannot be changed
after the cluster is first created, and `*clusterNetworkCIDR*` can only be
changed to be a larger network that still contains the original network. For
example, given the default value of *10.128.0.0/14*, you could change
`*clusterNetworkCIDR*` to *10.128.0.0/9* (i.e., the entire upper half of net
10) but not to *10.64.0.0/16*, because that does not overlap the original value.
====

[[configuring-the-pod-network-on-nodes]]
== Configuring the Pod Network on Nodes

The cluster administrators can control pod network settings on nodes by modifying
parameters in the `*networkConfig*` section of the
xref:../install_config/master_node_configuration.adoc#install-config-master-node-configuration[node configuration file]
(located at *_/etc/origin/node/node-config.yaml_* by default):

====
[source,yaml]
----
networkConfig:
  mtu: 1450 <1>
  networkPluginName: "redhat/openshift-ovs-subnet" <2>
----
<1> Maximum transmission unit (MTU) for the pod overlay network
<2> Set to *redhat/openshift-ovs-subnet* for the *ovs-subnet* plug-in or
*redhat/openshift-ovs-multitenant* for the *ovs-multitenant* plug-in
====

[[migrating-between-sdn-plugins]]
== Migrating Between SDN Plug-ins

If you are already using one SDN plug-in and want to switch to another:

. Change the `*networkPluginName*` parameter on all
xref:configuring-the-pod-network-on-masters[masters] and
xref:configuring-the-pod-network-on-nodes[nodes] in their configuration files.
ifdef::openshift-origin[]
. Restart the *origin-master* service on masters and the *origin-node* service
on nodes.
endif::[]
ifdef::openshift-enterprise[]
. Restart the *atomic-openshift-master* service on masters and the
*atomic-openshift-node* service on nodes.
endif::[]

When switching from the *ovs-subnet* to the *ovs-multitenant* OpenShift SDN plug-in,
all the existing projects in the cluster will be fully isolated (assigned unique VNIDs).
The cluster administrators can choose to xref:../admin_guide/managing_networking.adoc#admin-guide-pod-network[modify
the project networks] using the administrator CLI.

Check VNIDs by running:

----
$ oc get netnamespace
----

[[migrating-between-sdn-plugins-networkpolicy]]
=== Migrating from ovs-multitenant to ovs-networkpolicy

Before migrating from the *ovs-multitenant* plugin to the *ovs-networkpolicy*
plugin, ensure that every namespace has a unique `NetID`. This means that if you
have previously
xref:../admin_guide/managing_networking.adoc#joining-project-networks[joined projects
together] or
xref:../admin_guide/managing_networking.adoc#making-project-networks-global[made projects
global], you will need to undo that before switching to the *ovs-networkpolicy* plugin,
or the NetworkPolicy objects may not function correctly.

A helper script is available that fixes `NetID's`, creates NetworkPolicy objects
to isolate previously-isolated namespaces, and enables connections between
previously-joined namespaces.

Use the following steps to migrate to the *ovs-networkpolicy*
plugin, by using this helper script, while still running the *ovs-multitenant* plugin:

. Download the script from link:https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh[] and add the exexcution file permission:
+
[source, bash]
----
$ curl https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/migrate-network-policy.sh
$ chmod a+x migrate-network-policy.sh
----
. Run the script (requires the cluster administrator role).
+
[source, bash]
----
$ ./migrate-network-policy.sh
----

After running this script, every namespace is fully isolated from every other
namespace, therefore connection attempts between pods in different namespaces
will fail until you complete the migration to the *ovs-networkpolicy* plugin.

If you want newly-created namespaces to also have the same policies by default, you can set
xref:../admin_guide/managing_networking.adoc#admin-guide-networking-networkpolicy-setting-default[default
NetworkPolicy objects] to be created matching the `default-deny` and
`allow-from-global-namespaces` policies created by the migration script.

[NOTE]
====
In case of script failures or other errors, or if you later decide you want to
revert back to the *ovs-multitenant* plugin, you can use the
link:https://raw.githubusercontent.com/openshift/origin/master/contrib/migration/unmigrate-network-policy.sh[un-migration script]. This script undoes the changes made by the migration script and re-joins
previously-joined namespaces.
====

[[external-access-to-the-cluster-network]]
== External Access to the Cluster Network

If a host that is external to OpenShift requires access to the cluster network,
you have two options:

. Configure the host as an OpenShift node but mark it
xref:../admin_guide/manage_nodes.adoc#marking-nodes-as-unschedulable-or-schedulable[unschedulable]
so that the master does not schedule containers on it.
. Create a tunnel between your host and a host that is on the cluster network.

Both options are presented as part of a practical use-case in the documentation
for configuring xref:../install_config/routing_from_edge_lb.adoc#install-config-routing-from-edge-lb[routing from an
edge load-balancer to containers within OpenShift SDN].
